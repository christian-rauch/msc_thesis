% This file was created with JabRef 2.10b2.
% Encoding: UTF8


@InProceedings{Azad2011,
  Title                    = {{6-DoF} model-based tracking of arbitrarily shaped 3D objects},
  Author                   = {Azad, P. and Munch, D. and Asfour, T. and Dillmann, R.},
  Booktitle                = {Robotics and Automation (ICRA), 2011 IEEE International Conference on},
  Year                     = {2011},
  Month                    = {May},
  Pages                    = {5204-5209},

  Doi                      = {10.1109/ICRA.2011.5979950},
  ISSN                     = {1050-4729},
  Keywords                 = {cameras;feature extraction;image texture;object tracking;particle filtering (numerical methods);pose estimation;arbitrarily shaped 3D object tracking;camera;goal-directed imitation learning;image-based 6-DoF pose estimation;local feature straight line segment extraction;model-based tracking approach;object manipulation;particle filter based tracking approach;six-DoF model-based tracking;temporal information;textural information;Estimation;Graphics processing unit;Image edge detection;Rendering (computer graphics);Shape;Solid modeling;Three dimensional displays}
}

@Article{Bengio2009,
  Title                    = {Learning Deep Architectures for {AI}},
  Author                   = {Bengio, Yoshua},
  Journal                  = {Foundations and Trends in Machine Learning},
  Year                     = {2009},

  Month                    = jan,
  Number                   = {1},
  Pages                    = {1--127},
  Volume                   = {2},

  Acmid                    = {1658424},
  Address                  = {Hanover, MA, USA},
  Doi                      = {10.1561/2200000006},
  ISSN                     = {1935-8237},
  Issue_date               = {January 2009},
  Numpages                 = {127},
  Publisher                = {Now Publishers Inc.},
  Url                      = {http://dx.doi.org/10.1561/2200000006}
}

@Article{Bengio2014,
  Title                    = {Unsupervised Feature Learning and Deep Learning: {A} Review and New Perspectives},
  Author                   = {Yoshua Bengio and Aaron C. Courville and Pascal Vincent},
  Journal                  = {CoRR},
  Year                     = {2014},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/abs-1206-5538},
  Url                      = {http://arxiv.org/abs/1206.5538}
}

@Unpublished{Bengio-et-al-2015-Book,
  Title                    = {Deep Learning},
  Author                   = {Yoshua Bengio and Ian J. Goodfellow and Aaron Courville},
  Note                     = {Book in preparation for MIT Press},
  Year                     = {2015},

  Url                      = {http://www.iro.umontreal.ca/~bengioy/dlbook}
}

@InProceedings{Blum2012,
  Title                    = {A learned feature descriptor for object recognition in {RGB-D} data},
  Author                   = {M. Blum and Jost Tobias Springenberg and J. W\"{u}lfing and M. Riedmiller},
  Booktitle                = {Robotics and Automation (ICRA), 2012 IEEE International Conference on},
  Year                     = {2012},
  Month                    = {May},
  Pages                    = {1298-1303},

  Doi                      = {10.1109/ICRA.2012.6225188},
  ISSN                     = {1050-4729},
  Keywords                 = {feature extraction;image colour analysis;image representation;learning (artificial intelligence);object recognition;pose estimation;RGB-D image;RGB-D object dataset;concise representation;convolutional k-means descriptor;depth information;feature extraction;image processing pipeline;interest point detection;learned feature descriptor;machine learning community;object recognition;pose estimation;Accuracy;Feature extraction;Histograms;Object recognition;Training;Unsupervised learning;Vectors}
}

@InBook{Bo2013,
  Title                    = {Experimental Robotics: The 13th International Symposium on Experimental Robotics},
  Author                   = {Bo, Liefeng and Ren, Xiaofeng and Fox, Dieter},
  Chapter                  = {Unsupervised Feature Learning for {RGB-D} Based Object Recognition},
  Editor                   = {Desai, P. Jaydev and Dudek, Gregory and Khatib, Oussama and Kumar, Vijay},
  Pages                    = {387--402},
  Publisher                = {Springer International Publishing},
  Year                     = {2013},

  Address                  = {Heidelberg},

  Doi                      = {10.1007/978-3-319-00065-7_27},
  ISBN                     = {978-3-319-00065-7},
  Url                      = {http://dx.doi.org/10.1007/978-3-319-00065-7_27}
}

@InProceedings{Brethe2005,
  Title                    = {Determination of the Repeatability of a {Kuka} Robot Using the Stochastic Ellipsoid Approach},
  Author                   = {Brethe, J.-F. and Vasselin, E. and Lefebvre, D. and Dakyo, B.},
  Booktitle                = {Robotics and Automation, 2005. ICRA 2005. Proceedings of the 2005 IEEE International Conference on},
  Year                     = {2005},
  Month                    = {April},
  Pages                    = {4339-4344},

  Doi                      = {10.1109/ROBOT.2005.1570788},
  Keywords                 = {Repeatability;Robot Accuracy;Stochastic Ellipsoids;Actuators;Covariance matrix;Displays;Ellipsoids;Gaussian distribution;Kinematics;Random variables;Robotics and automation;Service robots;Stochastic processes;Repeatability;Robot Accuracy;Stochastic Ellipsoids}
}

@InProceedings{Chliveros2013,
  Title                    = {Robust Multi-hypothesis {3D} Object Pose Tracking},
  Author                   = {Chliveros, Georgios and Pateraki, Maria and Trahanias, Panos},
  Booktitle                = {Computer Vision Systems: 9th International Conference, ICVS 2013, St. Petersburg, Russia, July 16-18, 2013. Proceedings},
  Year                     = {2013},

  Address                  = {Berlin, Heidelberg},
  Editor                   = {Chen, Mei and Leibe, Bastian and Neumann, Bernd},
  Pages                    = {234--243},
  Publisher                = {Springer Berlin Heidelberg},

  Doi                      = {10.1007/978-3-642-39402-7_24},
  ISBN                     = {978-3-642-39402-7},
  Url                      = {http://dx.doi.org/10.1007/978-3-642-39402-7_24}
}

@InProceedings{Choi2013,
  Title                    = {{RGB-D} object tracking: A particle filter approach on {GPU}},
  Author                   = {Changhyun Choi and Christensen, H.I.},
  Booktitle                = {Intelligent Robots and Systems (IROS), 2013 IEEE/RSJ International Conference on},
  Year                     = {2013},
  Month                    = {Nov},
  Pages                    = {1084-1091},

  Doi                      = {10.1109/IROS.2013.6696485},
  ISSN                     = {2153-0858},
  Keywords                 = {feature extraction;graphics processing units;image colour analysis;image sequences;image texture;maximum likelihood estimation;object tracking;particle filtering (numerical methods);pose estimation;rendering (computer graphics);solid modelling;3D points feature;6-DOF object pose tracking;GPU;PCL;RGB-D object tracking;RGB-D sequence;a priori 3D mesh model;colors feature;degrees-of-freedom;geometric feature;graphics processing unit;object rendering;parallelized likelihood evaluation;particle filter approach;photometric feature;red-green-blue depth;surface normals feature;synthetic sequence;texture buffers;Cameras;Graphics processing units;Image color analysis;Rendering (computer graphics);Robots;Solid modeling;Three-dimensional displays}
}

@Article{Choi2012,
  Title                    = {Robust {3D} visual tracking using particle filtering on the special Euclidean group: A combined approach of keypoint and edge features},
  Author                   = {Choi, Changhyun and Christensen, Henrik I},
  Journal                  = {The International Journal of Robotics Research},
  Year                     = {2012},
  Number                   = {4},
  Pages                    = {498-519},
  Volume                   = {31},

  Abstract                 = {We present a 3D model-based visual tracking approach using edge and keypoint features in a particle filtering framework. Recently, particle-filtering-based approaches have been proposed to integrate multiple pose hypotheses and have shown good performance, but most of the work has made an assumption that an initial pose is given. To ameliorate this limitation, we employ keypoint features for initialization of the filter. Given 2D–3D keypoint correspondences, we randomly choose a set of minimum correspondences to calculate a set of possible pose hypotheses. Based on the inlier ratio of correspondences, the set of poses are drawn to initialize particles. After the initialization, edge points are employed to estimate inter-frame motions. While we follow a standard edge-based tracking, we perform a refinement process to improve the edge correspondences between sampled model edge points and image edge points. For better tracking performance, we employ a first-order autoregressive state dynamics, which propagates particles more effectively than Gaussian random walk models. The proposed system re-initializes particles by itself when the tracked object goes out of the field of view or is occluded. The robustness and accuracy of our approach is demonstrated using comparative experiments on synthetic and real image sequences.},
  Doi                      = {10.1177/0278364912437213},
  Eprint                   = {http://ijr.sagepub.com/content/31/4/498.full.pdf+html},
  Url                      = {http://ijr.sagepub.com/content/31/4/498.abstract}
}

@Article{Correll2016,
  Title                    = {Lessons from the Amazon Picking Challenge},
  Author                   = {Correll, Nikolaus and Bekris, Kostas E and Berenson, Dmitry and Brock, Oliver and Causo, Albert and Hauser, Kris and Okada, Kei and Rodriguez, Alberto and Romano, Joseph M and Wurman, Peter R},
  Journal                  = {arXiv preprint arXiv:1601.05484},
  Year                     = {2016}
}

@Article{Fan2010,
  Title                    = {Human Tracking Using Convolutional Neural Networks},
  Author                   = {Fan, Jialue and Xu, Wei and Wu, Ying and Gong, Yihong},
  Journal                  = {Trans. Neur. Netw.},
  Year                     = {2010},

  Month                    = oct,
  Number                   = {10},
  Pages                    = {1610--1623},
  Volume                   = {21},

  Acmid                    = {1892533},
  Address                  = {Piscataway, NJ, USA},
  Doi                      = {10.1109/TNN.2010.2066286},
  ISSN                     = {1045-9227},
  Issue_date               = {October 2010},
  Keywords                 = {Convolutional neural networks, convolutional neural networks, machine learning, visual tracking},
  Numpages                 = {14},
  Publisher                = {IEEE Press},
  Url                      = {http://dx.doi.org/10.1109/TNN.2010.2066286}
}

@Article{Farabet2013,
  Title                    = {Learning Hierarchical Features for Scene Labeling},
  Author                   = {Farabet, C. and Couprie, C. and Najman, L. and LeCun, Y.},
  Journal                  = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  Year                     = {2013},

  Month                    = {Aug},
  Number                   = {8},
  Pages                    = {1915-1929},
  Volume                   = {35},

  Doi                      = {10.1109/TPAMI.2012.231},
  ISSN                     = {0162-8828},
  Keywords                 = {feature extraction;image classification;image segmentation;image texture;shape recognition;transforms;trees (mathematics);Barcelona dataset;SIFT flow dataset;Stanford background dataset;contextual information capturing;dense feature vector extraction;hierarchical feature learning;image labeling;image pixel labeling;multiple size region encoding;multiscale convolutional network;near-record accuracy;object category;scene labeling;segmentation components;segmentation tree;shape information capturing;texture information capturing;Accuracy;Context;Feature extraction;Image edge detection;Image segmentation;Labeling;Vectors;Convolutional networks;deep learning;image classification;image segmentation;scene parsing}
}

@InProceedings{Gao2015,
  Title                    = {Loop closure detection for visual {SLAM} systems using deep neural networks},
  Author                   = {Xiang Gao and Tao Zhang},
  Booktitle                = {Control Conference (CCC), 2015 34th Chinese},
  Year                     = {2015},
  Month                    = {July},
  Pages                    = {5851-5856},

  Doi                      = {10.1109/ChiCC.2015.7260555},
  Keywords                 = {SLAM (robots);neurocontrollers;robot vision;bag-of-words;deep neural networks;loop closure detection;visual SLAM systems;visual simultaneous localization and mapping systems;Feature extraction;Machine learning;Neural networks;Simultaneous localization and mapping;Sparse matrices;Training;Visualization;Deep Neural Networks;Denoising Autoencoder;Loop Closure Detection;Simultaneous Localization and Mapping}
}

@InProceedings{Gupta2014,
  Title                    = {Learning Rich Features from {RGB-D} Images for Object Detection and Segmentation},
  Author                   = {Gupta, Saurabh and Girshick, Ross and Arbel{\'a}ez, Pablo and Malik, Jitendra},
  Booktitle                = {Proceedings of the 13th European Conference on Computer Vision},
  Year                     = {2014},

  Address                  = {Cham},
  Editor                   = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  Pages                    = {345--360},
  Publisher                = {Springer International Publishing},

  Chapter                  = {Computer Vision -- ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VII},
  Doi                      = {10.1007/978-3-319-10584-0_23},
  ISBN                     = {978-3-319-10584-0},
  Url                      = {http://dx.doi.org/10.1007/978-3-319-10584-0_23}
}

@InProceedings{Herzog2012,
  Title                    = {Template-based learning of grasp selection},
  Author                   = {A. Herzog and P. Pastor and M. Kalakrishnan and L. Righetti and T. Asfour and S. Schaal},
  Booktitle                = {Robotics and Automation (ICRA), 2012 IEEE International Conference on},
  Year                     = {2012},
  Month                    = {May},
  Pages                    = {2379-2384},

  Doi                      = {10.1109/ICRA.2012.6225271},
  ISSN                     = {1050-4729},
  Keywords                 = {dexterous manipulators;grippers;learning (artificial intelligence);manipulator kinematics;3D information;Barrett WAM arm;Willow Garage PR2;depth sensors;finger configuration;grasp configuration;grasp selection algorithm;grasp template;local shape descriptor;object grasp poses;personal robots;robotic arm kinematics;robotic hand kinematics;template-based learning;Grasping;Grippers;Libraries;Robots;Sensors;Shape;Tiles}
}

@InCollection{Jain2015,
  Title                    = {{MoDeep}: A Deep Learning Framework Using Motion Features for Human Pose Estimation},
  Author                   = {Jain, Arjun and Tompson, Jonathan and LeCun, Yann and Bregler, Christoph},
  Booktitle                = {Computer Vision -- ACCV 2014},
  Publisher                = {Springer International Publishing},
  Year                     = {2015},
  Editor                   = {Cremers, Daniel and Reid, Ian and Saito, Hideo and Yang, Ming-Hsuan},
  Pages                    = {302-315},
  Series                   = {Lecture Notes in Computer Science},
  Volume                   = {9004},

  Doi                      = {10.1007/978-3-319-16808-1_21},
  ISBN                     = {978-3-319-16807-4},
  Language                 = {English},
  Url                      = {http://dx.doi.org/10.1007/978-3-319-16808-1_21}
}

@Article{Jiu2014,
  Title                    = {Human body part estimation from depth images via spatially-constrained deep learning },
  Author                   = {Mingyuan Jiu and Christian Wolf and Graham Taylor and Atilla Baskurt},
  Journal                  = {Pattern Recognition Letters },
  Year                     = {2014},
  Note                     = {Depth Image Analysis },
  Pages                    = {122 - 129},
  Volume                   = {50},

  Doi                      = {http://dx.doi.org/10.1016/j.patrec.2013.09.021},
  ISSN                     = {0167-8655},
  Keywords                 = {Segmentation},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0167865513003577}
}

@Article{Koval2015,
  Title                    = {Pose estimation for planar contact manipulation with manifold particle filters},
  Author                   = {Koval, Michael C. and Pollard, Nancy S. and Srinivasa, Siddhartha S.},
  Journal                  = {The International Journal of Robotics Research},
  Year                     = {2015},
  Number                   = {7},
  Pages                    = {922-945},
  Volume                   = {34},

  Abstract                 = {We investigate the problem of using contact sensors to estimate the pose of an object during planar pushing by a fixed-shape hand. Contact sensors are unique because they inherently discriminate between “contact” and “no-contact” configurations. As a result, the set of object configurations that activates a sensor constitutes a lower-dimensional contact manifold in the configuration space of the object. This causes conventional state estimation methods, such as the particle filter, to perform poorly during periods of contact due to particle starvation. In this paper, we introduce the manifold particle filter as a principled way of solving the state estimation problem when the state moves between multiple manifolds of different dimensionality. The manifold particle filter avoids particle starvation during contact by adaptively sampling particles that reside on the contact manifold from the dual proposal distribution. We describe three techniques, one analytical and two sample-based, of sampling from the dual proposal distribution and compare their relative strengths and weaknesses. We present simulation results that show that all three techniques outperform the conventional particle filter in both speed and accuracy. In addition, we implement the manifold particle filter on a real robot and show that it successfully tracks the pose of a pushed object using commercially available tactile sensors.},
  Doi                      = {10.1177/0278364915571007},
  Eprint                   = {http://ijr.sagepub.com/content/34/7/922.full.pdf+html},
  Url                      = {http://ijr.sagepub.com/content/34/7/922.abstract}
}

@Article{Krainin2011,
  Title                    = {Manipulator and object tracking for in-hand {3D} object modeling},
  Author                   = {Krainin, Michael and Henry, Peter and Ren, Xiaofeng and Fox, Dieter},
  Journal                  = {The International Journal of Robotics Research},
  Year                     = {2011},
  Number                   = {11},
  Pages                    = {1311-1327},
  Volume                   = {30},

  Abstract                 = {Recognizing and manipulating objects is an important task for mobile robots performing useful services in everyday environments. While existing techniques for object recognition related to manipulation provide very good results even for noisy and incomplete data, they are typically trained using data generated in an offline process. As a result, they do not enable a robot to acquire new object models as it operates in an environment. In this paper we develop an approach to building 3D models of unknown objects based on a depth camera observing the robot’s hand while moving an object. The approach integrates both shape and appearance information into an articulated Iterative Closest Point approach to track the robot’s manipulator and the object. Objects are modeled by sets of surfels, which are small patches providing occlusion and appearance information. Experiments show that our approach provides very good 3D models even when the object is highly symmetric and lacks visual features and the manipulator motion is noisy. Autonomous object modeling represents a step toward improved semantic understanding, which will eventually enable robots to reason about their environments in terms of objects and their relations rather than through raw sensor data.},
  Doi                      = {10.1177/0278364911403178},
  Eprint                   = {http://ijr.sagepub.com/content/30/11/1311.full.pdf+html},
  Url                      = {http://ijr.sagepub.com/content/30/11/1311.abstract}
}

@InProceedings{Krull2015,
  Title                    = {{6-DOF} Model Based Tracking via Object Coordinate Regression},
  Author                   = {Krull, Alexander and Michel, Frank and Brachmann, Eric and Gumhold, Stefan and Ihrke, Stephan and Rother, Carsten},
  Booktitle                = {Computer Vision -- ACCV 2014: 12th Asian Conference on Computer Vision, Singapore, Singapore, November 1-5, 2014, Revised Selected Papers, Part IV},
  Year                     = {2015},

  Address                  = {Cham},
  Editor                   = {Cremers, Daniel and Reid, Ian and Saito, Hideo and Yang, Ming-Hsuan},
  Pages                    = {384--399},
  Publisher                = {Springer International Publishing},

  Doi                      = {10.1007/978-3-319-16817-3_25},
  ISBN                     = {978-3-319-16817-3},
  Url                      = {http://dx.doi.org/10.1007/978-3-319-16817-3_25}
}

@InProceedings{Lai2014,
  Title                    = {Unsupervised feature learning for {3D} scene labeling},
  Author                   = {Lai, K. and Liefeng Bo and Fox, D.},
  Booktitle                = {Robotics and Automation (ICRA), 2014 IEEE International Conference on},
  Year                     = {2014},
  Month                    = {May},
  Pages                    = {3050-3057},

  Doi                      = {10.1109/ICRA.2014.6907298},
  Keywords                 = {CAD;image colour analysis;solid modelling;unsupervised learning;virtual reality;3D point cloud data;3D scene labeling;CAD model;HMP3D classifiers;RGB-D images;RGB-D scenes dataset v.2;furniture pieces;hand-designed feature;hierarchical sparse coding technique;indoor scenes;learning features;object label;online database;scene labeling system;synthetic dataset;tabletop objects;unsupervised feature learning;virtual scenes;Dictionaries;Feature extraction;Labeling;Matching pursuit algorithms;Solid modeling;Three-dimensional displays;Videos}
}

@InProceedings{Lai2011,
  Title                    = {A large-scale hierarchical multi-view {RGB-D} object dataset},
  Author                   = {K. Lai and L. Bo and X. Ren and D. Fox},
  Booktitle                = {Robotics and Automation (ICRA), 2011 IEEE International Conference on},
  Year                     = {2011},
  Month                    = {May},
  Pages                    = {1817-1824},

  Doi                      = {10.1109/ICRA.2011.5980382},
  ISSN                     = {1050-4729},
  Keywords                 = {image colour analysis;image sensors;object recognition;robot vision;video signal processing;RGB-D camera;dataset collection procedure;instance detection;large-scale hierarchical multiview RGB-D object dataset;public image recognition;public image repositories;robotic object recognition;visual object category;Cameras;Object recognition;Robot sensing systems;Three dimensional displays;Video sequences;Videos;Visualization}
}

@Article{Liu2013,
  Title                    = {Online Unsupervised Feature Learning for Visual Tracking},
  Author                   = {Fayao Liu and Chunhua Shen and Ian D. Reid and Anton van den Hengel},
  Journal                  = {CoRR},
  Year                     = {2013},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/LiuSRH13},
  Url                      = {http://arxiv.org/abs/1310.1690}
}

@Article{Liu2015,
  Title                    = {Robust visual tracking via L0 regularized local low-rank feature learning},
  Author                   = {Liu, Risheng and Bai, Shanshan and Su, Zhixun and Zhang, Changcheng and Sun, Chunhai},
  Journal                  = {Journal of Electronic Imaging},
  Year                     = {2015},
  Number                   = {3},
  Pages                    = {033012},
  Volume                   = {24},

  Abstract                 = {Abstract.  Visual tracking is a fundamental task and has many applications in computer vision. We incorporate local dictionary and L0 regularized low-rank features into the particle filter framework to address this problem. Specifically, by developing an efficient L0 regularized sparse coding model to incrementally learn low-rank features for the tracking target and incorporating a local dictionary into low-rank features to build the observation model, we establish a robust online object tracking system. As a nontrivial byproduct, we also develop numerical algorithms to efficiently solve the resulting nonconvex optimization problems. Compared with conventional methods, which often directly use corrupted observations to form the dictionary, our low-rank feature-based dictionary successfully removes occlusions and exactly represents the intrinsic structure of the object. Furthermore, in contrast to the traditional holistic methods, the local strategy contains abundant partial and spatial information, thus enhancing the discrimination of our observation model. More importantly, the L0 norm-based hard sparse coding can successfully reduce the redundant information while preserving the intrinsic low-rank features of the target object, leading to a better appearance subspace updating scheme. Experimental results on challenging sequences show that our method consistently outperforms several state-of-the-art methods.},
  Doi                      = {10.1117/1.JEI.24.3.033012},
  ISBN                     = {1017-9909},
  Url                      = { http://dx.doi.org/10.1117/1.JEI.24.3.033012}
}

@InProceedings{Morwald2010,
  Title                    = {{BLORT} - The Blocks World Robotic Vision Toolbox},
  Author                   = {Thomas M\"{o}rwald and Johann Prankl and Andreas Richtsfeld and Michael Zillich and Markus Vincze},
  Booktitle                = {Proc. ICRA Workshop Best Practice in 3D Perception and Modeling for Mobile Manipulation},
  Year                     = {2010}
}

@InCollection{Neverova2014,
  Title                    = {Hand Segmentation with Structured Convolutional Learning},
  Author                   = {Neverova, Natalia and Wolf, Christian and Taylor, GrahamW. and Nebout, Florian},
  Booktitle                = {Computer Vision -- ACCV 2014},
  Publisher                = {Springer International Publishing},
  Year                     = {2015},
  Editor                   = {Cremers, Daniel and Reid, Ian and Saito, Hideo and Yang, Ming-Hsuan},
  Pages                    = {687-702},
  Series                   = {Lecture Notes in Computer Science},
  Volume                   = {9005},

  Doi                      = {10.1007/978-3-319-16811-1_45},
  ISBN                     = {978-3-319-16810-4},
  Language                 = {English},
  Url                      = {http://dx.doi.org/10.1007/978-3-319-16811-1_45}
}

@InProceedings{Newcombe2011,
  Title                    = {{KinectFusion}: Real-time dense surface mapping and tracking},
  Author                   = {Newcombe, Richard A. and Izadi, Shahram and Hilliges, Otmar and Molyneaux, David and Kim, David and Davison, Andrew J. and Kohi, Pushmeet and Shotton, Jamie and Hodges, Steve and Fitzgibbon, Andrew},
  Booktitle                = {Mixed and Augmented Reality (ISMAR), 2011 10th IEEE International Symposium on},
  Year                     = {2011},
  Month                    = {Oct},
  Pages                    = {127-136},

  Doi                      = {10.1109/ISMAR.2011.6092378},
  Keywords                 = {Cameras;Image reconstruction;Iterative closest point algorithm;Real time systems;Simultaneous localization and mapping;Surface reconstruction;Three dimensional displays;AR;Dense Reconstruction;Depth Cameras;GPU;Real-Time;SLAM;Tracking;Volumetric Representation}
}

@InProceedings{Pauwels2014b,
  Title                    = {Real-time object pose recognition and tracking with an imprecisely calibrated moving {RGB-D} camera},
  Author                   = {K. Pauwels and V. Ivan and E. Ros and S. Vijayakumar},
  Booktitle                = {Intelligent Robots and Systems (IROS 2014), International Conference on},
  Year                     = {2014},
  Month                    = {Sept},
  Pages                    = {2733-2740},

  Doi                      = {10.1109/IROS.2014.6942936},
  Keywords                 = {graphics processing units;image sensors;manipulators;object recognition;object tracking;pose estimation;real-time systems;robot vision;RGB-D camera;articulated robotic manipulator;camera object motion;computational capability;graphical capability;graphics processing units;position tracking;precise camera calibration;real-time object pose recognition;real-time object pose tracking;real-time system;soft constraint mechanism;target object motion;Cameras;Robot vision systems;Solid modeling;Three-dimensional displays;Tracking}
}

@Article{Pauwels2015,
  Title                    = {Real-time Pose Detection and Tracking of Hundreds of Objects},
  Author                   = {Pauwels, K. and Rubio, L. and Ros, E.},
  Journal                  = {IEEE Transactions on Circuits and Systems for Video Technology},
  Year                     = {2015},
  Number                   = {99},
  Pages                    = {1-1},
  Volume                   = {PP},

  Doi                      = {10.1109/TCSVT.2015.2430652},
  ISSN                     = {1051-8215},
  Keywords                 = {Adaptive optics;Feature extraction;Graphics processing units;Integrated optics;Optical imaging;Shape;Tracking;benchmarking;graphics processing unit (GPU);model-based object pose estimation;optical flow;real time;stereo}
}

@InProceedings{Pauwels2014,
  Title                    = {Real-Time Model-Based Articulated Object Pose Detection and Tracking with Variable Rigidity Constraints},
  Author                   = {K. Pauwels and L. Rubio and E. Ros},
  Booktitle                = {Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on},
  Year                     = {2014},
  Month                    = {June},
  Pages                    = {3994-4001},

  Doi                      = {10.1109/CVPR.2014.510},
  Keywords                 = {image representation;object detection;object tracking;pose estimation;articulation constraints;automatic system initialization;dense motion;depth cues;general articulated objects;independent object part detectors;iterative closest point approach;object pose detection;pose estimates;real-time detection;real-time tracking;relative motion;rigidization framework;six-degrees-of-freedom pose;Detectors;Joints;Kinematics;Mathematical model;Real-time systems;Tracking;Visualization;SIFT;articulated pose detection and tracking;augmented reality;graphics processing units;model-based pose estimation;occlusion;optical flow;real-time systems;stereo image processing;visibility-based rigidization}
}

@InProceedings{Ren2012,
  Title                    = {{RGB-(D)} scene labeling: Features and algorithms},
  Author                   = {X. Ren and L. Bo and D. Fox},
  Booktitle                = {Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on},
  Year                     = {2012},
  Month                    = {June},
  Pages                    = {2759-2766},

  Doi                      = {10.1109/CVPR.2012.6247999},
  ISSN                     = {1063-6919},
  Keywords                 = {image colour analysis;image segmentation;trees (mathematics);Microsoft Kinect;RGB-(D) scene labeling;RGB-D features;RGB-D perception;contextual modeling;indoor scenes;kernel descriptors;local similarities;outdoor scenes;patch descriptors;scene labeling research;segmentation tree;superpixel MRF;Accuracy;Context modeling;Image color analysis;Image segmentation;Kernel;Labeling;Vegetation}
}

@Article{Sunderhauf2015,
  Title                    = {On the Performance of {ConvNet} Features for Place Recognition},
  Author                   = {Niko S{\"{u}}nderhauf and Feras Dayoub and Sareh Shirazi and Ben Upcroft and Michael Milford},
  Journal                  = {CoRR},
  Year                     = {2015},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/SunderhaufDSUM15},
  Url                      = {http://arxiv.org/abs/1501.04158}
}

@InProceedings{Sanchez-Riera2014,
  Title                    = {A robust tracking algorithm for {3D} hand gesture with rapid hand motion through deep learning},
  Author                   = {Sanchez-Riera, J. and Yuan-Sheng Hsiao and Tekoing Lim and Kai-Lung Hua and Wen-Huang Cheng},
  Booktitle                = {Multimedia and Expo Workshops (ICMEW), 2014 IEEE International Conference on},
  Year                     = {2014},
  Month                    = {July},
  Pages                    = {1-6},

  Doi                      = {10.1109/ICMEW.2014.6890556},
  ISSN                     = {1945-7871},
  Keywords                 = {gesture recognition;image motion analysis;image registration;image sequences;learning (artificial intelligence);minimisation;object tracking;pose estimation;3D hand gesture;deep learning;frame rates;hand orientation estimation;hand pose estimation;initialization point;nonrigid model algorithm;objective function minimization;rapid hand motion;robust tracking algorithm;Cameras;Data models;Joints;Solid modeling;Three-dimensional displays;Tracking;Deep Learning;Gesture Recognition;Hand Model;Optimization;Tracking}
}

@InProceedings{Schmidt2015b,
  Title                    = {Depth-based tracking with physical constraints for robot manipulation},
  Author                   = {Schmidt, T. and Hertkorn, K. and Newcombe, R. and Marton, Z. and Suppa, M. and Fox, D.},
  Booktitle                = {Robotics and Automation (ICRA), 2015 IEEE International Conference on},
  Year                     = {2015},
  Month                    = {May},
  Pages                    = {119-126},

  Doi                      = {10.1109/ICRA.2015.7138989},
  Keywords                 = {grippers;manipulators;object detection;object tracking;robot vision;stability;state estimation;target tracking;DART;anthropomorphic hands;articulated objects;depth-based tracking;grasp planner;manipulation target tracking;object detection;physical constraints;real-time depth-only tracking;robot manipulation;robot manipulator target tracking;shared autonomy system;stability;state estimation;switching model;torque sensors;touch sensors;visual articulated object tracker;visual constraints;Calibration;Cameras;Joints;Robot sensing systems;Visualization}
}

@Article{Schmidt2015,
  Title                    = {{DART}: dense articulated real-time tracking with consumer depth cameras},
  Author                   = {Schmidt, Tanner and Newcombe, Richard and Fox, Dieter},
  Journal                  = {Autonomous Robots},
  Year                     = {2015},
  Number                   = {3},
  Pages                    = {239-258},
  Volume                   = {39},

  Doi                      = {10.1007/s10514-015-9462-z},
  ISSN                     = {0929-5593},
  Keywords                 = {Articulated model tracking; Signed distance function; Real-time vision; RGB-D},
  Language                 = {English},
  Publisher                = {Springer US},
  Url                      = {http://dx.doi.org/10.1007/s10514-015-9462-z}
}

@InProceedings{Sharp2015,
  Title                    = {Accurate, Robust, and Flexible Real-time Hand Tracking},
  Author                   = {Sharp, Toby and Keskin, Cem and Robertson, Duncan and Taylor, Jonathan and Shotton, Jamie and Kim, David and Rhemann, Christoph and Leichter, Ido and Vinnikov, Alon and Wei, Yichen and Freedman, Daniel and Kohli, Pushmeet and Krupka, Eyal and Fitzgibbon, Andrew and Izadi, Shahram},
  Booktitle                = {Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems},
  Year                     = {2015},

  Address                  = {New York, NY, USA},
  Pages                    = {3633--3642},
  Publisher                = {ACM},
  Series                   = {CHI '15},

  Acmid                    = {2702179},
  Doi                      = {10.1145/2702123.2702179},
  ISBN                     = {978-1-4503-3145-6},
  Keywords                 = {computer vision, depth camera, hand tracking},
  Location                 = {Seoul, Republic of Korea},
  Numpages                 = {10},
  Url                      = {http://doi.acm.org/10.1145/2702123.2702179}
}

@InBook{Shotton2013,
  Title                    = {Machine Learning for Computer Vision},
  Author                   = {Shotton, Jamie and Fitzgibbon, Andrew and Cook, Mat and Sharp, Toby and Finocchio, Mark and Moore, Richard and Kipman, Alex and Blake, Andrew},
  Chapter                  = {Real-Time Human Pose Recognition in Parts from Single Depth Images},
  Editor                   = {Cipolla, Roberto and Battiato, Sebastiano and Farinella, Maria Giovanni},
  Pages                    = {119--135},
  Publisher                = {Springer Berlin Heidelberg},
  Year                     = {2013},

  Address                  = {Berlin, Heidelberg},

  Doi                      = {10.1007/978-3-642-28661-2_5},
  ISBN                     = {978-3-642-28661-2},
  Url                      = {http://dx.doi.org/10.1007/978-3-642-28661-2_5}
}

@Article{Si2013,
  Title                    = {Learning {AND-OR} Templates for Object Recognition and Detection},
  Author                   = {Z. Si and S. C. Zhu},
  Journal                  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  Year                     = {2013},

  Month                    = {Sept},
  Number                   = {9},
  Pages                    = {2189-2205},
  Volume                   = {35},

  Doi                      = {10.1109/TPAMI.2013.35},
  ISSN                     = {0162-8828},
  Keywords                 = {generalisation (artificial intelligence);image matching;object detection;object recognition;unsupervised learning;AND-OR template learning;generalizability;graph compression procedure;hierarchical composition;hierarchical reconfigurable image template;information projection principle;object detection;object recognition;part articulation;part deformation;recursive block pursuit procedure;template matching;unsupervised learning;visual object;Animals;Face;Histograms;Image color analysis;Training;Unsupervised learning;Visualization;Deformable templates;image grammar;information projection;object recognition}
}

@InCollection{Socher2012,
  Title                    = {Convolutional-Recursive Deep Learning for {3D} Object Classification},
  Author                   = {Socher, Richard and Brody Huval and Bharath Bath and Manning, Christopher D and Andrew Y. Ng},
  Booktitle                = {Advances in Neural Information Processing Systems 25},
  Publisher                = {Curran Associates, Inc.},
  Year                     = {2012},
  Editor                   = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
  Pages                    = {656--664},

  Url                      = {http://papers.nips.cc/paper/4773-convolutional-recursive-deep-learning-for-3d-object-classification.pdf}
}

@InProceedings{Spong1996,
  Title                    = {Energy based control of a class of underactuated mechanical systems},
  Author                   = {Spong, Mark W},
  Booktitle                = {Proceedings of the 1996 IFAC World Congress},
  Year                     = {1996}
}

@Article{Sturm2009,
  Title                    = {Body schema learning for robotic manipulators from visual self-perception },
  Author                   = {Jürgen Sturm and Christian Plagemann and Wolfram Burgard},
  Journal                  = {Journal of Physiology-Paris },
  Year                     = {2009},
  Note                     = {Neurorobotics },
  Number                   = {3–5},
  Pages                    = {220 - 231},
  Volume                   = {103},

  Doi                      = {http://dx.doi.org/10.1016/j.jphysparis.2009.08.005},
  ISSN                     = {0928-4257},
  Keywords                 = {Robotics},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0928425709000461}
}

@Misc{drake,
  Title                    = {Drake: A planning, control, and analysis toolbox for nonlinear dynamical systems},

  Author                   = {Russ Tedrake},
  HowPublished             = {http://drake.mit.edu},
  Year                     = {2014},

  Url                      = {http://drake.mit.edu}
}

@Electronic{Tedrake2014,
  Title                    = {Underactuated Robotics: Algorithms for Walking, Running, Swimming, Flying, and Manipulation},
  Author                   = {Russ Tedrake},
  HowPublished             = {(Course Notes for MIT 6.832)},
  Url                      = {http://people.csail.mit.edu/russt/underactuated/},
  Year                     = {2014}
}

@InCollection{Tejani2014,
  Title                    = {Latent-class hough forests for {3D} object detection and pose estimation},
  Author                   = {Tejani, Alykhan and Tang, Danhang and Kouskouridas, Rigas and Kim, Tae-Kyun},
  Booktitle                = {Computer Vision--ECCV 2014},
  Publisher                = {Springer},
  Year                     = {2014},
  Pages                    = {462--477}
}

@InProceedings{Teuliere2010,
  Title                    = {Using multiple hypothesis in model-based tracking},
  Author                   = {Teuliere, C. and Marchand, E. and Eck, L.},
  Booktitle                = {Robotics and Automation (ICRA), 2010 IEEE International Conference on},
  Year                     = {2010},
  Month                    = {May},
  Pages                    = {4559-4565},

  Doi                      = {10.1109/ROBOT.2010.5509284},
  ISSN                     = {1050-4729},
  Keywords                 = {cameras;edge detection;image registration;image sequences;particle filtering (numerical methods);3D model-based tracking;camera pose;classic registration methods;edge projection;multiple low-level hypothesis;particle filtering framework;video sequences;Cameras;Filtering;Image edge detection;Particle tracking;Predictive models;Robot vision systems;Robotics and automation;Robustness;USA Councils;Video sequences}
}

@Article{Tompson2014,
  Title                    = {Real-Time Continuous Pose Recovery of Human Hands Using Convolutional Networks},
  Author                   = {Tompson, Jonathan and Stein, Murphy and LeCun, Y.un, Yann and Perlin, Ken},
  Journal                  = {ACM Transactions on Graphics (TOG)},
  Year                     = {2014},

  Month                    = sep,
  Number                   = {5},
  Pages                    = {169:1--169:10},
  Volume                   = {33},

  Acmid                    = {2629500},
  Address                  = {New York, NY, USA},
  Articleno                = {169},
  Doi                      = {10.1145/2629500},
  ISSN                     = {0730-0301},
  Issue_date               = {August 2014},
  Keywords                 = {Hand tracking, analysis-by-synthesis, markerless motion capture, neural networks},
  Numpages                 = {10},
  Publisher                = {ACM},
  Url                      = {http://doi.acm.org/10.1145/2629500}
}

@Article{Umeyama1991,
  Title                    = {Least-squares estimation of transformation parameters between two point patterns},
  Author                   = {Shinji Umeyama},
  Journal                  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  Year                     = {1991},

  Month                    = {Apr},
  Number                   = {4},
  Pages                    = {376-380},
  Volume                   = {13},

  Doi                      = {10.1109/34.88573},
  ISSN                     = {0162-8828},
  Keywords                 = {computer vision;error analysis;least squares approximations;parameter estimation;pattern recognition;computer vision,;least mean squared error;parameter estimation;pattern recognition;transformation parameters;two point patterns;Application software;Calibration;Cameras;Computer graphics;Computer vision;Image processing;Parameter estimation;Pattern recognition;Robot vision systems;Robotics and automation}
}

@InProceedings{Zhang2015,
  Title                    = {Visual Tracking with Convolutional Neural Network},
  Author                   = {Le Zhang and Ponnuthurai N. Suganthan},
  Booktitle                = {Proceedings of the IEEE International Conference on Systems, Man, and Cybernetics 2015},
  Year                     = {2015}
}

@Misc{DLN,
  Title                    = {Deep Learning website},
  HowPublished             = {http://deeplearning.net/},
  Month                    = {March},
  Year                     = {2015},

  Url                      = {http://deeplearning.net/}
}

