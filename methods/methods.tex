\chapter{Methods}

\section{Signed Distance Function}


\section{Gauss-Newton Algorithm}

The \textit{Gauss-Newton} algorithm is a method for finding the optimal set of parameters $\mathbf{p}$ that minimizes the sum of squares of non-linear vector-valued objective functions $e(\cdot)$
%
\begin{equation}
\hat{\mathbf{p}} = \arg\min_{p} \sum_{i=1}^N\left[ e_i(\mathbf{p})^2\right] .
\end{equation}

The Gauss-Newton algorithm is derived from the \textit{Newton} algorithm for finding roots and extremal points which itself is based on the Taylor series expansion. The Newton method for finding extremal points of the scalar objective function $f(x)$ is an iterative procedure
%
\begin{equation}
x_{t+1} = x_t - \frac{f^{(1)}(x_t)}{f^{(2)}(x_t)}
\label{eqn:newton_minimum}
\end{equation}
%
starting at an initial state $x_0$ using the first- and second-order derivatives ($f^{(1)}$, $f^{(2)}$) of the objective function.

For finding the minimum of the function $e(\cdot)$ with respect to a parameter vector $\mathbf{p}$, the first- and second-order partial derivatives are used. In particular, the first- and second-order derivatives in \ref{eqn:newton_minimum} are replaced by the gradient $\nabla e(\mathbf{p})$ and the Hessian $\nabla^2 e(\mathbf{p})$ of the objective function $e$.
The partial derivatives of the vector-valued function $e(\cdot)$ with respect to the parameter vector $\mathbf{p}$ are aggregated in the \textit{Jacobian} matrix $J\in \mathbb{R}^{i \times j}$ with elements
%
\begin{equation}
J_{i,j} = \frac{\partial e_i(\mathbf{p})}{\partial p_j} .
\end{equation}
%
Hence,
\begin{align}
\nabla e(\mathbf{p}) &= J^\top e(\mathbf{p}) \label{eqn:gn_gradient}\\
H(e) = \nabla^2 e(\mathbf{p}) &= J^\top J +  \sum_{i=1} e_i(\mathbf{p}) \nabla^2 e_i(\mathbf{p}) \label{eqn:gn_hessian_full}.
\end{align}

For small $e$, the Hessian $H(e)$ can be approximated by
\begin{equation}
H(e) \approx J^\top J \label{eqn:gn_hessian}
\end{equation}
%
neglecting the second-order derivatives. This approximation is used in the Gauss-Newton algorithm and the parameter update step becomes
%
\begin{align}
\mathbf{p}_{t+1} &= \mathbf{p}_{t} - \Delta\mathbf{p} \\
&= \mathbf{p}_{t} - H(e)^{-1} \cdot \nabla e(\mathbf{p}) \\
&= \mathbf{p}_{t} - \left(J^\top J\right)^{-1} \cdot J^\top e(\mathbf{p}) \label{eqn:gn_param_update}.
\end{align}

The parameter update $\Delta\mathbf{p}$ is proportionally driven by the gradient of the objective function and inverse proportional by the approximation of the Hessian.


\section{Prior Information}

Prior information can be used in the Gauss-Newton algorithm by modulating the Hessian approximation and the gradient towards the desired state. This is achieved by selecting an objective function and calculating its partial derivatives with respect to the parameter vector. These partial derivatives are used in the Jacobian as shown in equations \ref{eqn:gn_gradient} and \ref{eqn:gn_hessian}.

The full parameter vector in DART consists of the 6D pose of the root frame and the $N$ joints positions:
\begin{equation}
\mathbf{p} = \begin{bmatrix}
x & y & z & \phi & \theta & \psi & | & q_1 & \cdots & q_N
\end{bmatrix}
\end{equation}

with $x$, $y$, $z$ for the translation, $\phi$, $\theta$, $\psi$ for the orientation in radiant Euler angles, and $q_1 \dots q_N$ the individual joint position values.


\subsection{Known Frame Pose}

For constraining the translation of the root frame in between optimization iterations, the parameter update vector $\Delta\mathbf{p}$ must be computed such that its first 6 elements for the pose are $0$. From equation \ref{eqn:gn_param_update}
\begin{align}
\Delta\mathbf{p} &= \left(J^\top J\right)^{-1} \cdot J^\top e(\mathbf{p}) = \begin{bmatrix}
\mathbf{0} & | & q_1 & \cdots & q_N
\end{bmatrix} \\
J^\top e &= J^\top J \cdot \begin{bmatrix}
\mathbf{0} & | & q_1 & \cdots & q_N
\end{bmatrix}
\end{align}

Cancelling out the pose update can be achieved by applying the prior on the final blended Hessian and gradient just before the final parameter update is computed. In particular the fixed frame prior computes the parameter update $\Delta\mathbf{p}$ from the blended Hessian and gradient, cancels out the first 6 elements for the pose, and computes the new gradient. Instead of blending this gradient ($J^\top e$) into the final gradient, the global gradient is overwritten resulting in enforcing no frame pose update.


\subsection{Reported Joint Positions}

We use the reported state of the robot in a way, that we can drag the estimated solution away from possibly distracting perceptions towards the state perceived through the joint encoders.
%
Three different kind of objective functions have been selected for this purpose which compute a weighted deviation of the estimated joint configuration $\mathbf{q}$ from reported joint configuration $\mathbf{r}$. For all objective functions, $r_i$ and $q_i$ denote the individual joint positions in their corresponding configuration vector.

\begin{enumerate}
\item Weighted L2 norm of the estimated joint position deviation:
\begin{equation}
e_1 = w \cdot \lVert \mathbf{r} - \mathbf{q} \rVert = w \cdot \sqrt{\sum_{i=1}^N (r_i - q_i)^2} \label{eqn:objf_weightedL2}
\end{equation}
with scalar weight $w>0$.

\item L2 norm of the weighted joint deviation:
\begin{equation}
e_2 = \lVert w \cdot (\mathbf{r} - \mathbf{q}) \rVert = \sqrt{\sum_{i=1}^N w \cdot (r_i - q_i)^2} \label{eqn:objf_L2ofweighted}
\end{equation}
with the same scalar weight $w>0$ for all joint deviations.

\item Sum of squares of individually weighted joints:
\begin{equation}
e_3 = \lvert \mathbf{r} - \mathbf{q} \rvert^\top Q \lvert \mathbf{r} - \mathbf{q} \rvert = \sum_{i=1}^N \left[ (r_i-q_i) \cdot \sum_{j=1}^N (r_j-q_j) \cdot \omega_{i,j} \right] \label{eqn:objf_indiv_weighted}
\end{equation}
with the weight matrix $Q\in\mathbf{R}^{N\times N}$ and its elements $\omega_{i,j}$. As a error metric, the weight matrix $Q$ must be \emph{positive-semidefinite}. That is, $\forall \mathbf{x}\in\mathbf{R}^n: x^\top Q x \geq 0$.
\end{enumerate}

Whereas the first two objective functions (\ref{eqn:objf_weightedL2} and \ref{eqn:objf_L2ofweighted}) use a common scalar weight for all joint deviations, the third objective function (\ref{eqn:objf_indiv_weighted}) can apply different weights per joint and hence enables to penalize whole parts of the kinematic chain more precise.

The respective partial derivatives for the Jacobian are:
\begin{enumerate}
\item Weighted L2 norm of the estimated joint position deviation
\begin{equation}
J_{ji} = \frac{\partial e_1}{\partial q_i} = -w \frac{r_i - q_i}{\lVert \mathbf{r} - \mathbf{q} \rVert}
\end{equation}

\item L2 norm of the weighted joint deviation:
\begin{equation}
J_{ji} = \frac{\partial e_2}{\partial q_i} = -w^2 \frac{r_i - q_i}{\lVert w \cdot (\mathbf{r} - \mathbf{q}) \rVert}
\end{equation}

\item Sum of squares of individually weighted joints:
\begin{equation}
J_{ji} = \frac{\partial e_3}{\partial q_i} = - \left[ \sum_{j=1}^N d_jq_{i,j}\right] + \left[ \sum_{j=1 \neq i}^N d_jq_{j,i}\right]
\end{equation}
\end{enumerate}


\section{Ground Truth Data Collection}

